<!DOCTYPE html>
<html>

<head>
    <!-- Meta, title, CSS, favicons, etc. -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="JingHao Zhang Home">
    <meta name="keywords" content="JingHao Zhang,Homepage">
    <meta name="author" content="JingHao Zhang">
    <title>JingHao Zhang Home</title>
    <link rel="stylesheet" type="text/css" href="https://unpkg.com/dmego-home-page@latest/assets/css/onlinewebfonts.css"/>
    <link rel="stylesheet" type="text/css" href="https://unpkg.com/dmego-home-page@latest/assets/css/vno.css">
    <link rel="stylesheet" type="text/css" href="https://unpkg.com/dmego-home-page@latest/assets/css/iconfont.css">
    <link rel="apple-touch-icon" href="https://unpkg.com/dmego-home-page@latest/apple-touch-icon.png">
    <link rel="icon" href="https://unpkg.com/dmego-home-page@latest/favicon.ico">
</head>

<body>
    <span class="mobile btn-mobile-menu">
        <i class="social iconfont icon-list btn-mobile-menu__icon"></i>
        <i class="social iconfont icon-angleup btn-mobile-close__icon hidden"></i>
    </span>
    <header id="panel" class="panel-cover">
        <div class="panel-main">
            <div class="panel-main__inner panel-inverted">
                <div class="panel-main__content">
                    <div class="ih-item circle effect right_to_left">            
                        <a href="#" title="" class="blog-button">
                            <div class="img"><img src="https://i.328888.xyz/2023/05/06/iTYs3a.png" alt="img" class="js-avatar iUp profilepic"></div>
                            <div class="info iUp">
                                <div class="info-back">
                                    <h2> 
                                        
                                            Fighting
                                        
                                    </h2>
                                    <p>
                                        
                                            2023 · 突破
                                        
                                    </p>
                                </div>
                            </div>
                        </a>
                    </div>
                    <h1 class="panel-cover__title panel-title iUp">
                        <a href="#" title="JingHao Zhang Home">JingHao Zhang</a>
                    </h1>
                    <p class="panel-cover__subtitle panel-subtitle iUp">Code Create Life</p>
                    <hr class="panel-cover__divider iUp" />
                    <p id="description" class="panel-cover__description iUp">如何得与凉风约, 不共尘沙一并来!
                        <br/>
                        <strong>-「中牟道中」</strong>
                    </p>
                    <div class="navigation-wrapper iUp">
                        <div>
                            <nav class="cover-navigation cover-navigation--primary">
                                <ul class="navigation">
                                    <li class="navigation__item">
                                        <a href="/" class="blog-button">首页</a>
                                    </li>
                                    <li class="navigation__item">
                                        <a href="#section-1" class="blog-button" target="_blank">博客</a>
                                    </li>
                                    <li class="navigation__item">
                                        <a href="./publication.html" class="blog-button" target="_blank">简历</a>
                                    </li>
                                    <li class="navigation__item">
                                        <a href="/" class="blog-button" target="_blank">关于</a>
                                    </li>
                                </ul>
                            </nav>
                        </div>
                        <div class="iUp">
                            <nav class="cover-navigation navigation--social">
                                <ul class="navigation">
                                    <li class="navigation__item">
                                        <a href="https://github.com/JingHao99/" title="github" target="_blank">
                                            <i class='social iconfont icon-github'></i>
                                            <span class="label">github</span>
                                        </a>
                                    </li>
                                    <li class="navigation__item">
                                        <a href="/" title="cnblogs" target="_blank">
                                            <i class='social iconfont icon-cnblogs'></i>
                                            <span class="label">cnblogs</span>
                                        </a>
                                    </li>

                                    <li class="navigation__item">
                                        <a href="/" title="zhihu" target="_blank">
                                            <i class='social iconfont icon-zhihu'></i>
                                            <span class="label">zhihu</span>
                                        </a>
                                    </li>
                                    <li class="navigation__item">
                                        <!-- 加密的邮箱地址, 请自行修改, 或者改用下方的明文方式 -->
                                        <a href="javascript:decryptEmail('amhhb3poYW5nQG1haWwudXN0Yy5lZHUuY24=');" title="email">
                                        <!-- <a href="jhaozhang@mail.ustc.edu.cn" title="email"> -->
                                            <i class='social iconfont icon-email'></i>
                                            <span class="label">email</span>
                                        </a>
                                    </li>
                                </ul>
                            </nav>
                        </div>
                    </div>
                </div>
            </div>
            <div class="panel-cover--overlay cover-slate"></div>
        </div>
        <!-- <div class="beian iUp">
            <a class="icp" href="http://www.miitbeian.gov.cn/publish/query/indexFirst.action" target="_blank">XXX</a>
            <a class="gwab" href="http://www.beian.gov.cn/portal/recordQuery" target="_blank">XXX</a>
        </div> -->
        <div class="remark iUp">
            <p class="power">Powered By 
                <a href="https://github.com/features/actions" target="_blank"> GitHub Actions </a> And 
                <a href="https://hitokoto.cn/" target="_blank"> Hitokoto </a>
            </p>
        </div>
    </header>
    
    <!-- Main Content -->
    <main class="main minh-100vh">
        <!-- Section -->
        <section class="py-0 overflow-hidden text-center">
            <div class="container">
                <div class="row h-100vh justify-content-center align-items-center">
                    <div class="move d-none d-lg-block">
                        <a href="#section-1" class="text-white" data-smooth-scroll data-smooth-scroll-hash="false">
                            <i class="zmdi zmdi-long-arrow-down zmdi-hc-2x"></i>
                        </a>
                    </div>
                </div>
            </div>
        </section>
        <!-- End of Section -->

        <!-- Section -->
        <section id="section-1" class="pt-7 pb-6">
            <div class="container">
                <h3 class="my-0 fs-1 fw-medium text-primary text-uppercase text-center text-lg-left">Recent</h3>
                <h2 class="mb-5 fw-medium text-secondary text-center text-lg-left">Projects</h2>

                <div class="row justify-content-center">

                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <a href="./project/vtoonify/index.html" target="_blank" class="mb-3 d-block position-relative">
                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-secondary rounded text-white">
                                <i class="fas fa-link fa-2x"></i>
                            </div>
                            <img src="./assets/img/teasers/vtoonify.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                        </a>
                    </div>
                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <p>
                            <span class="text-primary">VToonify: Controllable High-Resolution Portrait Video Style Transfer </span> 
                            <br />
                            <span class="text-500">
                            S. Yang, L. Jiang, Z. Liu, C. C. Loy <br /> 
                            ACM Transactions on Graphics, 2022 <strong>(SIGGRAPH ASIA - TOG)</strong><br />    
                            </span>
                            [<a href="https://arxiv.org/abs/2209.11224" target="_blank"><span class="text-muted">arXiv</span></a>]
                            [<a href="./project/vtoonify/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                            [<a href="https://www.youtube.com/watch?v=0_OmVhDgYuY" target="_blank"><span class="text-muted">YouTube</span></a>]
                            [<a href="https://huggingface.co/spaces/PKUWilliamYang/VToonify" target="_blank"><span class="text-muted">Demo</span></a>]
                        </p>
                        <p class="text-justify">
                             We present a novel VToonify framework for controllable high-resolution portrait video style transfer. VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to complete face regions with natural motions in the output.
                        </p>
                    </div>

                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <a href="https://frozenburning.github.io/projects/text2light/" target="_blank" class="mb-3 d-block position-relative">
                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-secondary rounded text-white">
                                <i class="fas fa-link fa-2x"></i>
                            </div>
                            <img src="./assets/img/teasers/text2light.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                        </a>
                    </div>
                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <p>
                            <span class="text-primary">Text2Light: Zero-Shot Text-Driven HDR Panorama Generation </span> 
                            <br />
                            <span class="text-500">
                            Z. Chen, G. Wang, Z. Liu <br /> 
                            ACM Transactions on Graphics, 2022 <strong>(SIGGRAPH ASIA - TOG)</strong><br />    
                            </span>
                            [<a href="https://arxiv.org/abs/2209.09898" target="_blank"><span class="text-muted">arXiv</span></a>]
                            [<a href="https://frozenburning.github.io/projects/text2light/" target="_blank"><span class="text-muted">Project Page</span></a>]
                            [<a href="https://www.youtube.com/watch?v=XDx6tOHigPE" target="_blank"><span class="text-muted">YouTube</span></a>]
                            [<a href="https://colab.research.google.com/github/FrozenBurning/Text2Light/blob/master/text2light.ipynb" target="_blank"><span class="text-muted">Demo</span></a>]
                        </p>
                        <p class="text-justify">
                             We propose a zero-shot text-driven framework, Text2Light, to generate 4K+ resolution HDRIs without paired training data.
                        </p>
                    </div>

                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <a href="./project/maskclip/index.html" target="_blank" class="mb-3 d-block position-relative">
                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-secondary rounded text-white">
                                <i class="fas fa-link fa-2x"></i>
                            </div>
                            <img src="./assets/img/teasers/maskclip.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                        </a>
                    </div>
                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <p>
                            <span class="text-primary">Extract Free Dense Labels from CLIP </span> 
                            <br />
                            <span class="text-500">
                            C. Zhou, C. C. Loy, B. Dai <br /> 
                            European Conference on Computer Vision, 2022 <strong>(ECCV, Oral)</strong><br />    
                            </span>
                            [<a href="https://arxiv.org/abs/2112.01071" target="_blank"><span class="text-muted">arXiv</span></a>]
                            [<a href="./project/maskclip/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                        </p>
                        <p class="text-justify">
                             We examine the intrinsic potential of CLIP for pixel-level dense prediction, specifically in semantic segmentation. With minimal modification, we show that MaskCLIP yields compelling segmentation results on open concepts across various datasets in the absence of annotations and fine-tuning. By adding pseudo labeling and self-training, MaskCLIP+ surpasses SOTA transductive zero-shot semantic segmentation methods by large margins.
                        </p>
                    </div>

                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <a href="./project/ovdetr/index.html" target="_blank" class="mb-3 d-block position-relative">
                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-secondary rounded text-white">
                                <i class="fas fa-link fa-2x"></i>
                            </div>
                            <img src="./assets/img/teasers/ovdetr.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                        </a>
                    </div>
                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <p>
                            <span class="text-primary">Open-Vocabulary DETR with Conditional Matching </span> 
                            <br />
                            <span class="text-500">
                            Y. Zang, W. Li, K. Zhou, C. Huang, C. C. Loy <br /> 
                            European Conference on Computer Vision, 2022 <strong>(ECCV, Oral)</strong><br />    
                            </span>
                            [<a href="https://arxiv.org/abs/2203.11876" target="_blank"><span class="text-muted">arXiv</span></a>]
                            [<a href="./project/ovdetr/index.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                        </p>
                        <p class="text-justify">
                             We propose a novel open-vocabulary detector based on DETR, which once trained, can detect any object given its class name or an exemplar image. This first end-to-end Transformer-based open-vocabulary detector achieves non-trivial improvements over current state of the arts.
                        </p>
                    </div>


                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <a href="https://caizhongang.github.io/projects/HuMMan/" target="_blank" class="mb-3 d-block position-relative">
                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-secondary rounded text-white">
                                <i class="fas fa-link fa-2x"></i>
                            </div>
                            <img src="./assets/img/teasers/humman.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                        </a>
                    </div>
                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <p>
                            <span class="text-primary">HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling </span> 
                            <br />
                            <span class="text-500">
                            Z. Cai, D. Ren, A. Zeng, Z. Lin, T. Yu, W. Wang, X. Fan, Y. Gao, Y. Yu, L. Pan, F. Hong, M. Zhang, C. C. Loy, L. Yang, Z. Liu <br /> 
                            European Conference on Computer Vision, 2022 <strong>(ECCV, Oral)</strong><br />    
                            </span>
                            [<a href="https://arxiv.org/abs/2204.13686" target="_blank"><span class="text-muted">arXiv</span></a>]
                            [<a href="https://caizhongang.github.io/projects/HuMMan/" target="_blank"><span class="text-muted">Project Page</span></a>]
                            [<a href="https://www.youtube.com/watch?v=Q_lxIrV3UgE" target="_blank"><span class="text-muted">YouTube</span></a>]
                        </p>
                        <p class="text-justify">
                             We contribute HuMMan, a large-scale multi-modal 4D human dataset with 1000 human subjects, 400k sequences and 60M frames. HuMMan has several appealing properties: 1) multi-modal data and annotations including color images, point clouds, keypoints, SMPL parameters, and textured meshes; 2) popular mobile device is included in the sensor suite; 3) a set of 500 actions, designed to cover fundamental movements; 4) multiple tasks such as action recognition, pose estimation, parametric human recovery, and textured mesh reconstruction are supported and evaluated.
                        </p>
                    </div>


                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <a href="https://hongfz16.github.io/projects/AvatarCLIP.html" target="_blank" class="mb-3 d-block position-relative">
                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-secondary rounded text-white">
                                <i class="fas fa-link fa-2x"></i>
                            </div>
                            <img src="./assets/img/teasers/avatarclip.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                        </a>
                    </div>
                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <p>
                            <span class="text-primary">AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars </span> 
                            <br />
                            <span class="text-500">
                            F. Hong, M. Zhang, L. Pan, Z. Cai, L. Yang, Z. Liu <br /> 
                            ACM Transactions on Graphics, 2022 <strong>(SIGGRAPH - TOG)</strong><br />    
                            </span>
                            [<a href="https://arxiv.org/abs/2205.08535" target="_blank"><span class="text-muted">arXiv</span></a>]
                            [<a href="https://hongfz16.github.io/projects/AvatarCLIP.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                        </p>
                        <p class="text-justify">
                             We propose AvatarCLIP, a zero-shot text-driven framework for 3D avatar generation and animation. Unlike professional software that requires expert knowledge, AvatarCLIP empowers layman users to customize a 3D avatar with the desired shape and texture, and drive the avatar with the described motions using solely natural languages.
                        </p>
                    </div>

                    <div class="col-lg-4 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <a href="https://yumingj.github.io/projects/Text2Human.html" target="_blank" class="mb-3 d-block position-relative">
                            <div class="position-absolute d-flex justify-content-center align-items-center w-100 h-100 bg-secondary rounded text-white">
                                <i class="fas fa-link fa-2x"></i>
                            </div>
                            <img src="./assets/img/teasers/text2human.jpg" alt="" class="position-relative img-fluid w-100 shadow-lg rounded opacity-1-hover" />
                        </a>
                    </div>
                    <div class="col-lg-8 mb-4 mb-lg-6" data-aos="fade-up" data-aos-duration="1000">
                        <p>
                            <span class="text-primary">Text2Human: Text-Driven Controllable Human Image Generation </span> 
                            <br />
                            <span class="text-500">
                            Y. Jiang, S. Yang, H. Qiu, W. Wu, C. C. Loy, Z. Liu <br /> 
                            ACM Transactions on Graphics, 2022 <strong>(SIGGRAPH - TOG)</strong><br />    
                            </span>
                            [<a href="https://arxiv.org/abs/2205.15996" target="_blank"><span class="text-muted">arXiv</span></a>]
                            [<a href="https://yumingj.github.io/projects/Text2Human.html" target="_blank"><span class="text-muted">Project Page</span></a>]
                            [<a href="https://www.youtube.com/watch?v=yKh4VORA_E0" target="_blank"><span class="text-muted">YouTube</span></a>]
                        </p>
                        <p class="text-justify">
                             We present a text-driven controllable framework, Text2Human, for high-quality and diverse human generation. We synthesize full-body human images starting from a given human pose with two dedicated steps: 1) With some texts describing the shapes of clothes, the given human pose is first translated to a human parsing map. 2) The final human image is then generated by providing the system with more attributes about the textures of clothes.
                        </p>
                    </div>

                </div>
            </div>
        </section>
        <!-- End of Section -->      
    </main>
    <!-- End of Main Content -->
    <script type="text/javascript" src="https://unpkg.com/dmego-home-page@latest/assets/js/main.js"></script>
    <script type="text/javascript" src="./assets/json/images.json?cb=getBingImages"></script>
</body>
</html>

<!-- Google Analytics (请自行删除) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4L0GELWRT2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4L0GELWRT2');
</script>
